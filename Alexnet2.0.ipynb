{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432940d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "# np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95ca664",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (227, 227, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351cf360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the path to the main directory containing the subfolders\n",
    "# dataset_dir = \"H:\\\\Casptone_datset\"\n",
    "\n",
    "# # Get the list of subfolder names\n",
    "# class_names = sorted(os.listdir(dataset_dir))\n",
    "\n",
    "# # Initialize lists to store the filenames and corresponding classes\n",
    "# filenames = []\n",
    "# classes = []\n",
    "\n",
    "# # Iterate over the subfolders and collect the filenames and classes\n",
    "# for class_name in class_names:\n",
    "#     class_dir = os.path.join(dataset_dir, class_name)\n",
    "#     if os.path.isdir(class_dir):\n",
    "#         file_list = os.listdir(class_dir)\n",
    "#         filenames.extend([os.path.join(class_dir, file_name) for file_name in file_list])\n",
    "#         classes.extend([class_name] * len(file_list))\n",
    "\n",
    "# # Split the data into train, test, and validation sets\n",
    "# train_files, test_val_files, train_classes, test_val_classes = train_test_split(filenames, classes, test_size=0.3, random_state=42)\n",
    "# test_files, val_files, test_classes, val_classes = train_test_split(test_val_files, test_val_classes, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataFrames for train, test, and validation sets\n",
    "# train_df = pd.DataFrame({\"filename\": train_files, \"class\": train_classes})\n",
    "# test_df = pd.DataFrame({\"filename\": test_files, \"class\": test_classes})\n",
    "# val_df = pd.DataFrame({\"filename\": val_files, \"class\": val_classes})\n",
    "\n",
    "\n",
    "# Set the path to the main directory containing the subfolders\n",
    "dataset_dir = \"H:/15 classes 6k Images\"\n",
    "\n",
    "# Get the list of subfolder names\n",
    "class_names = sorted(os.listdir(dataset_dir))\n",
    "\n",
    "# Initialize lists to store the filenames and corresponding classes\n",
    "filenames = []\n",
    "classes = []\n",
    "\n",
    "# Iterate over the subfolders and collect the filenames and classes\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        file_list = os.listdir(class_dir)\n",
    "        filenames.extend([os.path.join(class_dir, file_name) for file_name in file_list])\n",
    "        classes.extend([class_name] * len(file_list))\n",
    "\n",
    "# Split the data into train, test, and validation sets\n",
    "train_files, test_val_files, train_classes, test_val_classes = train_test_split(filenames, classes, test_size=0.3, random_state=42)\n",
    "test_files, val_files, test_classes, val_classes = train_test_split(test_val_files, test_val_classes, test_size=0.33, random_state=42)\n",
    "\n",
    "# Create DataFrames for train, test, and validation sets\n",
    "train_df = pd.DataFrame({\"filename\": train_files, \"class\": train_classes})\n",
    "test_df = pd.DataFrame({\"filename\": test_files, \"class\": test_classes})\n",
    "val_df = pd.DataFrame({\"filename\": val_files, \"class\": val_classes})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b7e71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff38f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 50\n",
    "epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e84c896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4200 validated image filenames belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generate batches of augmented training data\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"class\",\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa2008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1206 validated image filenames belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generate batches of test data\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"class\",\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b82ea539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 594 validated image filenames belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generate batches of validation data\n",
    "val_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"class\",\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc74248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4200 validated image filenames belonging to 15 classes.\n",
      "Found 1206 validated image filenames belonging to 15 classes.\n",
      "Found 594 validated image filenames belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(train_df, x_col=\"filename\", y_col=\"class\",\n",
    "                                                    target_size=input_shape[:2], batch_size=batch_size, class_mode=\"categorical\")\n",
    "test_generator = test_datagen.flow_from_dataframe(test_df, x_col=\"filename\", y_col=\"class\",\n",
    "                                                  target_size=input_shape[:2], batch_size=batch_size, class_mode=\"categorical\")\n",
    "val_generator = val_datagen.flow_from_dataframe(val_df, x_col=\"filename\", y_col=\"class\",\n",
    "                                                target_size=input_shape[:2], batch_size=batch_size, class_mode=\"categorical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f30ab642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = len(train_generator.class_indices)\n",
    "# # Model architecture\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# # First Convolutional Layer\n",
    "# model.add(Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape))\n",
    "# model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# # Second Convolutional Layer\n",
    "# model.add(Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# # Third Convolutional Layer\n",
    "# model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# # Fourth Convolutional Layer\n",
    "# model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# # Fifth Convolutional Layer\n",
    "# model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# # Flatten the feature maps\n",
    "# model.add(Flatten())\n",
    "\n",
    "# # Fully Connected Layers\n",
    "# model.add(Dense(4096, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(4096, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# # Output Layer\n",
    "# model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f87300ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "# Define the number of classes\n",
    "num_classes = len(train_generator.class_indices)\n",
    "model = Sequential()\n",
    "\n",
    "# First Convolutional Layer\n",
    "model.add(Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Second Convolutional Layer\n",
    "model.add(Conv2D(256, (5, 5), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Third Convolutional Layer\n",
    "model.add(Conv2D(384, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "# Fourth Convolutional Layer\n",
    "model.add(Conv2D(384, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "# Fifth Convolutional Layer\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Flatten the feature maps\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully Connected Layers\n",
    "model.add(Dense(4096, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0790e8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 27, 27, 96)       384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 13, 13, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 6, 6, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 15)                61455     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,345,231\n",
      "Trainable params: 58,344,015\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2fbfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b059f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "es=EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a03e7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# Define a callback to save the model with the best accuracy\n",
    "mc = ModelCheckpoint('H:\\\\Casptone_datset\\\\best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "254ba1f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "84/84 [==============================] - 717s 8s/step - loss: 65.5452 - accuracy: 0.4748 - val_loss: 28.5463 - val_accuracy: 0.1127\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - 781s 9s/step - loss: 15.3383 - accuracy: 0.6943 - val_loss: 16.5361 - val_accuracy: 0.0964\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - 639s 8s/step - loss: 6.5355 - accuracy: 0.7867 - val_loss: 8.5071 - val_accuracy: 0.2218\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - 611s 7s/step - loss: 4.0980 - accuracy: 0.8086 - val_loss: 5.7048 - val_accuracy: 0.3418\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - 654s 8s/step - loss: 3.1714 - accuracy: 0.8514 - val_loss: 4.8363 - val_accuracy: 0.4636\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - 590s 7s/step - loss: 2.6498 - accuracy: 0.8402 - val_loss: 4.7601 - val_accuracy: 0.4109\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - 594s 7s/step - loss: 2.5454 - accuracy: 0.8583 - val_loss: 3.5122 - val_accuracy: 0.5582\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - 743s 9s/step - loss: 2.4006 - accuracy: 0.8600 - val_loss: 3.2285 - val_accuracy: 0.6382\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - 752s 9s/step - loss: 2.2876 - accuracy: 0.8567 - val_loss: 9.9998 - val_accuracy: 0.2000\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - 811s 10s/step - loss: 2.2642 - accuracy: 0.8657 - val_loss: 2.8104 - val_accuracy: 0.6509\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - 762s 9s/step - loss: 1.9816 - accuracy: 0.8867 - val_loss: 3.0384 - val_accuracy: 0.6473\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - 826s 10s/step - loss: 1.9519 - accuracy: 0.8852 - val_loss: 5.1253 - val_accuracy: 0.4291\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - 821s 10s/step - loss: 1.6934 - accuracy: 0.9031 - val_loss: 5.4157 - val_accuracy: 0.3982\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - 830s 10s/step - loss: 1.9955 - accuracy: 0.8893 - val_loss: 2.1020 - val_accuracy: 0.8127\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - 853s 10s/step - loss: 1.7103 - accuracy: 0.9045 - val_loss: 3.2492 - val_accuracy: 0.5527\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - 852s 10s/step - loss: 1.6959 - accuracy: 0.9117 - val_loss: 1.9479 - val_accuracy: 0.8327\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - 838s 10s/step - loss: 1.5051 - accuracy: 0.9164 - val_loss: 2.0343 - val_accuracy: 0.8291\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - 840s 10s/step - loss: 1.6900 - accuracy: 0.9117 - val_loss: 1.5527 - val_accuracy: 0.9073\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - 838s 10s/step - loss: 1.3859 - accuracy: 0.9245 - val_loss: 3.9275 - val_accuracy: 0.4800\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - 835s 10s/step - loss: 1.4754 - accuracy: 0.9274 - val_loss: 1.4074 - val_accuracy: 0.9055\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - 846s 10s/step - loss: 1.3479 - accuracy: 0.9298 - val_loss: 3.5219 - val_accuracy: 0.4727\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - 838s 10s/step - loss: 1.3715 - accuracy: 0.9271 - val_loss: 1.9662 - val_accuracy: 0.7673\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - 837s 10s/step - loss: 1.4127 - accuracy: 0.9226 - val_loss: 2.1351 - val_accuracy: 0.7564\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - 810s 10s/step - loss: 1.4073 - accuracy: 0.9214 - val_loss: 1.6273 - val_accuracy: 0.8600\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - 831s 10s/step - loss: 1.3229 - accuracy: 0.9429 - val_loss: 1.5020 - val_accuracy: 0.8327\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - 836s 10s/step - loss: 1.2549 - accuracy: 0.9298 - val_loss: 2.4666 - val_accuracy: 0.6564\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - 828s 10s/step - loss: 1.3381 - accuracy: 0.9343 - val_loss: 1.3345 - val_accuracy: 0.9182\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - 671s 8s/step - loss: 1.0603 - accuracy: 0.9581 - val_loss: 1.1314 - val_accuracy: 0.8836\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - 297s 4s/step - loss: 0.8674 - accuracy: 0.9614 - val_loss: 1.2852 - val_accuracy: 0.8273\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - 297s 4s/step - loss: 0.9620 - accuracy: 0.9398 - val_loss: 1.2346 - val_accuracy: 0.8855\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - 298s 4s/step - loss: 1.3232 - accuracy: 0.9286 - val_loss: 2.4924 - val_accuracy: 0.6655\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - 300s 4s/step - loss: 1.2449 - accuracy: 0.9467 - val_loss: 2.0161 - val_accuracy: 0.8145\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - 297s 4s/step - loss: 1.1132 - accuracy: 0.9486 - val_loss: 1.1133 - val_accuracy: 0.9400\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - 298s 4s/step - loss: 1.0799 - accuracy: 0.9412 - val_loss: 3.2700 - val_accuracy: 0.6582\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - 300s 4s/step - loss: 1.2143 - accuracy: 0.9431 - val_loss: 1.9563 - val_accuracy: 0.7127\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - 299s 4s/step - loss: 1.0088 - accuracy: 0.9543 - val_loss: 1.1404 - val_accuracy: 0.9218\n",
      "Epoch 37/50\n",
      "84/84 [==============================] - 300s 4s/step - loss: 0.9999 - accuracy: 0.9576 - val_loss: 1.3204 - val_accuracy: 0.8655\n",
      "Epoch 38/50\n",
      "84/84 [==============================] - 298s 4s/step - loss: 1.0133 - accuracy: 0.9510 - val_loss: 1.1903 - val_accuracy: 0.8945\n",
      "Epoch 39/50\n",
      "84/84 [==============================] - 300s 4s/step - loss: 0.9877 - accuracy: 0.9610 - val_loss: 1.0889 - val_accuracy: 0.9055\n",
      "Epoch 40/50\n",
      "84/84 [==============================] - 300s 4s/step - loss: 0.8834 - accuracy: 0.9538 - val_loss: 1.7166 - val_accuracy: 0.7255\n",
      "Epoch 41/50\n",
      "84/84 [==============================] - 299s 4s/step - loss: 1.1829 - accuracy: 0.9340 - val_loss: 1.3972 - val_accuracy: 0.8836\n",
      "Epoch 42/50\n",
      "84/84 [==============================] - 299s 4s/step - loss: 1.1392 - accuracy: 0.9421 - val_loss: 1.5945 - val_accuracy: 0.7909\n",
      "Epoch 43/50\n",
      "84/84 [==============================] - 299s 4s/step - loss: 1.0421 - accuracy: 0.9557 - val_loss: 1.4619 - val_accuracy: 0.8564\n",
      "Epoch 44/50\n",
      "84/84 [==============================] - 299s 4s/step - loss: 0.9202 - accuracy: 0.9593 - val_loss: 1.0878 - val_accuracy: 0.9109\n",
      "Epoch 45/50\n",
      "84/84 [==============================] - 298s 4s/step - loss: 0.9033 - accuracy: 0.9581 - val_loss: 1.9741 - val_accuracy: 0.6655\n",
      "Epoch 46/50\n",
      "84/84 [==============================] - 300s 4s/step - loss: 0.9230 - accuracy: 0.9617 - val_loss: 3.6120 - val_accuracy: 0.5055\n",
      "Epoch 47/50\n",
      "84/84 [==============================] - 299s 4s/step - loss: 0.8955 - accuracy: 0.9648 - val_loss: 1.2743 - val_accuracy: 0.8636\n",
      "Epoch 48/50\n",
      "84/84 [==============================] - 298s 4s/step - loss: 0.8006 - accuracy: 0.9681 - val_loss: 0.8896 - val_accuracy: 0.9491\n",
      "Epoch 49/50\n",
      "84/84 [==============================] - 300s 4s/step - loss: 0.7990 - accuracy: 0.9650 - val_loss: 0.9627 - val_accuracy: 0.9291\n",
      "Epoch 50/50\n",
      "84/84 [==============================] - 298s 4s/step - loss: 1.0248 - accuracy: 0.9412 - val_loss: 1.5255 - val_accuracy: 0.8618\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "H=model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8a4f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 47s 2s/step - loss: 1.5303 - accuracy: 0.8583\n",
      "Test Loss: 1.530256986618042\n",
      "Test Accuracy: 0.8583333492279053\n"
     ]
    }
   ],
   "source": [
    "# test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.n // test_generator.batch_size)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "710e6282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 27s 860ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_generator, steps=test_generator.n // test_generator.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dae8a8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step\n",
      "Predicted class: Parking\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Load the image\n",
    "image_path = \"H:\\\\images_data\\\\test\\\\No left turn\\\\1_output_0_3370.png\"\n",
    "img = Image.open(image_path)\n",
    "img = img.resize((227, 227))  # Resize the image to match the input size of VGG16\n",
    "\n",
    "# Convert the image to RGB mode\n",
    "img = img.convert(\"RGB\")\n",
    "\n",
    "# Preprocess the image\n",
    "img_array = np.array(img)\n",
    "img_array = img_array / 255.0  # Normalize the pixel values\n",
    "img_array = preprocess_input(img_array)  # Apply VGG16 preprocessing\n",
    "\n",
    "# Reshape the image array to match the expected input shape of the model\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Make the prediction\n",
    "predictions = model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "print(\"Predicted class:\", class_names[predicted_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dff0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a377a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b70a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
